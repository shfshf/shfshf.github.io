---
layout:     post
title:      Kashgari
subtitle:   æç®€ä¸”å¼ºå¤§çš„NLPæ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ†ç±»ä¸æ ‡æ³¨ 
date:       2019-12-26
author:     SHF
header-img: img/post-bg-debug.png
catalog: true
tags:
    - NLP
    - NER
    - DeepLearning
---

# Kashgariæ¦‚è¿°

__Kashgari æ˜¯ä¸€ä¸ªæç®€ä¸”å¼ºå¤§çš„ NLP æ¡†æ¶ï¼Œå¯ç”¨äºæ–‡æœ¬åˆ†ç±»å’Œæ ‡æ³¨çš„å­¦ä¹ ï¼Œç ”ç©¶åŠéƒ¨ç½²ä¸Šçº¿__

* **æ–¹ä¾¿æ˜“ç”¨** Kashgari æä¾›äº†ç®€æ´ç»Ÿä¸€çš„ API å’Œå®Œå–„çš„æ–‡æ¡£ï¼Œä½¿å…¶éå¸¸æ–¹ä¾¿æ˜“ç”¨ã€‚
* **å†…ç½®è¿ç§»å­¦ä¹ æ¨¡å—** Kashgari é€šè¿‡æä¾› `BertEmbedding`, `GPT2Embedding`, `WordEmbedding` ç­‰ç‰¹å¾æå–ç±»ï¼Œ
æ–¹ä¾¿åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å®ç°è¿ç§»å­¦ä¹ ã€‚
* **æ˜“æ‰©å±•** Kashgari æä¾›ç®€ä¾¿çš„æ¥å£å’Œç»§æ‰¿å…³ç³»ï¼Œè‡ªè¡Œæ‰©å±•æ–°çš„æ¨¡å‹ç»“æ„éå¸¸æ–¹ä¾¿ã€‚
* **å¯ç”¨äºç”Ÿäº§** é€šè¿‡æŠŠ Kashgari æ¨¡å‹å¯¼å‡ºä¸º `SavedModel` æ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨ TensorFlow Serving æ¨¡å—æä¾›æœåŠ¡ï¼Œç›´æ¥åœ¨çº¿ä¸Šç¯å¢ƒä½¿ç”¨ã€‚
* Kashgari çš„[*Github*](https://github.com/BrikerMan/Kashgari)ç½‘å€
* Kashgari æœ¬è´¨ä¸Šæ˜¯åŸºäºkerasçš„é«˜å±‚å°è£…ï¼Œæ‰€ä»¥ä½¿ç”¨æ ¼å¼ï¼Œæ€§èƒ½æ–¹é¢æ˜¯ç›¸å·®æ— å¤šçš„ã€‚

__Kashgari ä¸»è¦ç”¨æ¥æ–‡æœ¬åˆ†ç±»ä¸æ ‡æ³¨ï¼Œç›®å‰æœ¬æ–‡ä¸»è¦ä»‹ç»äº†æ–‡æœ¬æ ‡æ³¨ï¼ˆNERï¼‰çš„ä½¿ç”¨ ä»¥åŠ*å¡«å‘*ï¼Œä»¥åæœ‰æ—¶é—´ä¼šæŠŠ*åˆ†ç±»*çš„å‘å¡«ä¸Š__

# NER å‘½åå®ä½“è¯†åˆ«

## å®‰è£…

Kashgari é¡¹ç›®åŸºäº `Tensorflow 1.14.0` å’Œ `Python 3.6+` 

ç›®å‰ä¹Ÿæœ‰ *tf2.0* çš„ç‰ˆæœ¬æ”¯æŒï¼Œä½†ç›®å‰æ²¡æœ‰ *tf1.4* ç¨³å®š

```
pip install kashgari
# CPU
pip install tensorflow==1.14.0
# GPU
pip install tensorflow-gpu==1.14.0
```

## è½½å…¥æ•°æ®

è¿™é‡Œmodelçš„æ•°æ®æ ¼å¼æ˜¯å›ºå®šçš„ï¼Œå¦‚æœè¦ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Œéœ€è¦è®¤çœŸæ•ˆéªŒè‡ªå·±çš„æ•°æ®æ ¼å¼ã€‚

æˆ‘é‡åˆ°çš„ä¸¤ä¸ªé—®é¢˜ï¼š
1. ç©ºæ ¼ \t ä¸ ' 'åŒºåˆ«
2. entityå®ä½“å€¼ç¼ºçœçš„ï¼Œåœ¨å¯¹åº”çš„æ ‡æ³¨ä½ç½®ä¸Š
3. æ ‡æ³¨è§„èŒƒæ”¯æŒBIOï¼ˆæ¨èï¼‰ã€BIEOã€SIEOã€SIOã€

```
# åŠ è½½å†…ç½®æ•°æ®é›†ï¼Œæ­¤å¤„å¯ä»¥æ›¿æ¢æˆè‡ªå·±çš„æ•°æ®é›†ï¼Œä¿è¯æ ¼å¼ä¸€è‡´å³å¯

# load internal data
from kashgari.corpus import ChineseDailyNerCorpus

train_x, train_y = ChineseDailyNerCorpus.load_data('train')
valid_x, valid_y = ChineseDailyNerCorpus.load_data('validate')
test_x, test_y = ChineseDailyNerCorpus.load_data('test')

# load own data
from kashgari.corpus import DataReader

train_x, train_y = DataReader().read_conll_format_file('./data/surround/train.txt')
valid_x, valid_y = DataReader().read_conll_format_file('data/surround/dev.txt')
test_x, test_y = DataReader().read_conll_format_file('./data/surround/test.txt')
print(f"train data count: {len(train_x)}")
print(f"validate data count: {len(valid_x)}")
print(f"test data count: {len(test_x)}")

```

## åˆ›å»º embedding

### create Bare embedding

åœ¨ BareEmbedding ä¸­å‚æ•°`sequence_length`æœ‰ä¸‰ç§æ¨¡å¼ï¼š
* 'auto': è¿™é‡Œé»˜è®¤çš„æ˜¯ 'auto'ï¼Œå³å°†è¯­æ–™åº“é•¿åº¦çš„ 95ï¼… ä½œä¸ºåºåˆ—é•¿åº¦
* 'variable': æ¨¡å‹è¾“å…¥å½¢çŠ¶å°†è®¾ç½®ä¸º None ï¼Œå¯ä»¥å¤„ç†å„ç§é•¿åº¦çš„è¾“å…¥ï¼Œå®ƒå°†ä½¿ç”¨æ¯æ‰¹ä¸­æœ€å¤§åºåˆ—çš„é•¿åº¦ä½œä¸ºåºåˆ—é•¿åº¦
* 'intå€¼': å¦‚æœä½¿ç”¨æ•´æ•°ï¼Œåˆ™å‡è®¾ **50** è¾“å…¥è¾“å‡ºåºåˆ—é•¿åº¦å°†è®¾ç½®ä¸º **50**

è¿™é‡Œæˆ‘é€‰æ‹©çš„æ˜¯ 'variable' æ¨¡å¼ï¼Œå› ä¸ºåœ¨ä¸‹é¢çš„modelæ¨ç†ï¼ˆtensorflow servingï¼‰çš„æ—¶å€™ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”è¯­å¥çš„é•¿åº¦å€¼ï¼Œ
ä¸ä¼šæœ‰å¤šä½™çš„ 'PAD'å€¼ã€‚

```
from kashgari.embeddings import BareEmbedding

bare_embed = BareEmbedding(task=kashgari.LABELING,
                           sequence_length='variable',
                           embedding_size=300)
```

### create BERT embedding

å¦‚æœä½¿ç”¨ `BERTEmbedding` ç¼–ç çš„æ—¶å€™ï¼Œ`sequence_length`æ¨¡å¼é»˜è®¤è®¾ç½®çš„åªæ¥å—'intå€¼' ğŸ™‰ 

BERT_PATH ä¸­çš„ `'<bert-model-folder>'`æ˜¯æˆ‘ä»¬ä¸‹è½½çš„ **bert** é¢„è®­ç»ƒæ¨¡å‹ï¼ˆpreâˆ’trainingï¼‰çš„æ–‡ä»¶è·¯å¾„ï¼Œ
æ¯”å¦‚ `BERT_PATH = './models/bert_base_models/chinese_L-12_H-768_A-12/'`

```
from kashgari.embeddings import BERTEmbedding

BERT_PATH = '<bert-model-folder>'
bert_embed = BERTEmbedding(BERT_PATH,
                           task=kashgari.LABELING,
                           sequence_length=50)
```

__å¯¹äºä½¿ç”¨ GPT-2 embedding çš„æ–¹æ³•ä¸ BERT embedding æ˜¯ç±»ä¼¼çš„ã€‚__

__openAI GPT-2 å®˜æ–¹æä¾›çš„ä¸¤ç§é¢„è®­ç»ƒæ¨¡å‹ï¼šsmallï¼ˆ117Mï¼‰ï¼Œmediumï¼ˆ345Mï¼‰ï¼Œlarge ç‰ˆæœ¬å¹¶æ²¡æœ‰å‘å¸ƒğŸ™‰__

__ä½†å¯¹äºä¸­æ–‡çš„æ”¯æŒå¹¶ä¸è‰¯å¥½ï¼Œæœ‰å…´è¶£ç ”ç©¶ä¸­æ–‡ç‰ˆçš„ GPT-2 æ¨¡å‹çš„å°ä¼™ä¼´è¯·ç§»æ­¥[GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese)__

##  é€‰æ‹© model 

Kashgari æä¾›çš„ modelæœ‰: 

`CNN_LSTM_Model`, `BiLSTM_Model`, `BiGRU_Model` or `BiGRU_CRF_Model`, `BiLSTM_CRF_Model`

```
from kashgari.tasks.labeling import BiLSTM_CRF_Model

model = BiLSTM_CRF_Model(bare_embed)
# or model = BiLSTM_CRF_Model(bert_embed)
# or model = BiLSTM_CRF_Model(gpt2_embed)

```

## å¯è§†åŒ–é€‰æ‹©

è¿™æ˜¯ Kashgari å†…ç½®å›è°ƒå‡½æ•°ï¼Œä¼šåœ¨è®­ç»ƒè¿‡ç¨‹è®¡ç®—ç²¾ç¡®åº¦ï¼Œå¬å›ç‡å’Œ F1

```
from kashgari.callbacks import EvalCallBack

# check  $ tensorboard --logdir logs
tf_board_callback = keras.callbacks.TensorBoard(log_dir='./logs', update_freq=1000)
eval_callback = EvalCallBack(kash_model=model,
                             valid_x=valid_x,
                             valid_y=valid_y,
                             step=5)

```

## è®­ç»ƒæ¨¡å‹ 

### é€‰æ‹©å¯è§†åŒ–

```
model.fit(train_x,
          train_y,
          x_validate=valid_x,
          y_validate=valid_y,
          epochs=5,
          batch_size=32,
          callbacks=[eval_callback, tf_board_callback])
```

### ä¸é€‰æ‹©å¯è§†åŒ–

```
model.fit(train_x,
          train_y,
          x_validate=valid_x,
          y_validate=valid_y,
          epochs=1,
          batch_size=32)
```

##  è¯„ä¼°æ¨¡å‹

è¯„ä¼°æ¨¡å‹çš„å…·ä½“ metrics ï¼Œè¯·æŸ¥çœ‹ç›¸å…³çš„ api æ–‡æ¡£

```
model.evaluate(test_x, test_y)
```

##  ä¿å­˜æ¨¡å‹

å¯ä»¥é€‰æ‹©å…¶ä»–çš„åº“å‡½æ•°æ¥ä¿å­˜æ¨¡å‹ï¼Œè¿™é‡Œé€‰æ‹©çš„å‡½æ•°æ˜¯æ–¹ä¾¿ä¸‹é¢ `tensorflow sering` çš„æ¨ç†ï¼›

æ³¨æ„ `model_path` çš„è·¯å¾„æ ¼å¼ã€‚

```
from kashgari import utils

utils.convert_to_saved_model(model,
                             model_path='./save_models/ner',
                             version='1')
```
##  results test

ä¸ºäº†å¿«é€ŸéªŒè¯ model çš„ç»“æœï¼š

æ•°æ®ä½¿ç”¨çš„æ˜¯å†…ç½®çš„äººåæ—¥æŠ¥çš„æ•°æ®é›†ï¼ˆChineseDailyNerCorpusï¼‰

ä½¿ç”¨çš„ç¼–ç æ˜¯ `BareEmbedding`ï¼Œ`epoch`åªè®¾äº†**1**è½®çš„ç»“æœã€‚

```
train data count: 20864
validate data count: 2318
test data count: 4636
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, None)]            0         
_________________________________________________________________
layer_embedding (Embedding)  (None, None, 300)         1050000   
_________________________________________________________________
layer_blstm (Bidirectional)  (None, None, 256)         439296    
_________________________________________________________________
layer_dense (Dense)          (None, None, 64)          16448     
_________________________________________________________________
layer_crf_dense (Dense)      (None, None, 8)           520       
_________________________________________________________________
layer_crf (CRF)              (None, None, 8)           64        
=================================================================
Total params: 1,506,328
Trainable params: 1,506,328
Non-trainable params: 0
_________________________________________________________________
653/653 [==============================] - 350s 536ms/step - loss: 15.2126 - accuracy: 0.9662 - val_loss: 157.9817 - val_accuracy: 0.8512
           precision    recall  f1-score   support

      ORG     0.4330    0.4050    0.4185      2185
      LOC     0.6217    0.5831    0.6018      3658
      PER     0.6569    0.6604    0.6586      1864

micro avg     0.5778    0.5513    0.5642      7707
macro avg     0.5767    0.5513    0.5636      7707

```

##  æ¨¡å‹æ¨ç†

### docker é•œåƒæŒ‚è½½æ¨¡å‹

é€šè¿‡ docker å®‰è£…`tensorflow/serving` é•œåƒï¼Œç„¶åå°†æœåŠ¡æŒ‚è½½åˆ°æœ¬åœ°çš„ 8501 ç«¯å£ï¼Œ

å…¶ä¸­`"/Users/shf/PycharmProjects/Kashgari/save_models:/models"`æ˜¯æœ¬åœ°ç”Ÿæˆæ¨¡å‹çš„è·¯å¾„æ ¼å¼

__å¦‚æœæ˜¯è¿œç¨‹æœåŠ¡å™¨ä¸Šè¿›è¡Œæ¨¡å‹æ¨ç†ï¼Œè¿‡ç¨‹æ˜¯ç›¸ä¼¼çš„__

```
docker run -t --rm -p 8501:8501 -v "/Users/shf/PycharmProjects/Kashgari/save_models:/models" -e MODEL_NAME=ner tensorflow/serving

```

### è½½å…¥æ¨¡å‹

```
from kashgari import utils

processor = utils.load_processor(model_path='./save_models/ner/1')
```

### é¢„å¤„ç†æ•°æ®

```
test_x = ['ä¸­', 'å›½', 'é˜Ÿ', 'å°†', 'äº', 'åŒ—', 'äº¬', 'å¯¹', 'é˜µ', 'æ¥', 'ç€', 'ä¸­', 'ä¸œ', 'çš„', 'å™', 'åˆ©', 'äºš', 'é˜Ÿ', 'ã€‚']
tensor = processor.process_x_dataset(data=[test_x])
```

å¦‚æœä½¿ç”¨ `BERTEmbedding`ï¼Œéœ€è¦å¤šä¸€æ­¥ç»è¿‡ä¸‹é¢çš„é¢„å¤„ç†

```
# if you using BERT, you need to reformat tensor first
# ------ Only for BERT Embedding Start --------
import numpy as np

tensor = [{
   "Input-Token:0": i.tolist(),
   "Input-Segment:0": np.zeros(i.shape).tolist()
} for i in tensor]
# ------ Only for BERT Embedding End ----------
```

### è¿›è¡Œé¢„æµ‹

æ³¨æ„ Bare embedding ä¸ BERT embeddingä½¿ç”¨åŒºåˆ«

```
import requests

r = requests.post("http://localhost:8501/v1/models/ner:predict", json={"instances": tensor.tolist()}) # Bare embedding
# or r = requests.post("http://localhost:8501/v1/models/ner:predict", json={"instances": tensor})  # BERT embedding
preds = r.json()['predictions']

# Convert result back to labels
pred = np.array(preds).argmax(-1)
labels = processor.reverse_numerize_label_sequences(pred)
print(test_x)
print(labels)

```

### results predict
```
['ä¸­', 'å›½', 'é˜Ÿ', 'å°†', 'äº', 'åŒ—', 'äº¬', 'å¯¹', 'é˜µ', 'æ¥', 'ç€', 'ä¸­', 'ä¸œ', 'çš„', 'å™', 'åˆ©', 'äºš', 'é˜Ÿ', 'ã€‚']
[['B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O']]

```
